HYPATIA - Adversarial Machine Learning Testing Platform
References and Resources
@misc{goodfellow2015explainingharnessingadversarialexamples,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1412.6572}, 
}
@misc{brendel2018decisionbasedadversarialattacksreliable,
      title={Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models}, 
      author={Wieland Brendel and Jonas Rauber and Matthias Bethge},
      year={2018},
      eprint={1712.04248},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1712.04248}, 
}
@misc{carlini2017evaluatingrobustnessneuralnetworks,
      title={Towards Evaluating the Robustness of Neural Networks}, 
      author={Nicholas Carlini and David Wagner},
      year={2017},
      eprint={1608.04644},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1608.04644}, 
}
@misc{carlini2017evaluatingrobustnessneuralnetworks,
      title={Towards Evaluating the Robustness of Neural Networks}, 
      author={Nicholas Carlini and David Wagner},
      year={2017},
      eprint={1608.04644},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1608.04644}, 
}
@misc{moosavidezfooli2016deepfoolsimpleaccuratemethod,
      title={DeepFool: a simple and accurate method to fool deep neural networks}, 
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
      year={2016},
      eprint={1511.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.04599}, 
}
@misc{papernot2015limitationsdeeplearningadversarial,
      title={The Limitations of Deep Learning in Adversarial Settings}, 
      author={Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt Fredrikson and Z. Berkay Celik and Ananthram Swami},
      year={2015},
      eprint={1511.07528},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1511.07528}, 
}
@misc{bhagoji2017exploringspaceblackboxattacks,
      title={Exploring the Space of Black-box Attacks on Deep Neural Networks}, 
      author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},
      year={2017},
      eprint={1712.09491},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.09491}, 
}
@INPROCEEDINGS{10829076,
  author={Nirmal, Santosh and Patil, Pramod},
  booktitle={2024 International Conference on Intelligent Systems and Advanced Applications (ICISAA)}, 
  title={Evasion Attack against CNN-Adaboost Electricity Theft Detection in Smart Grid}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Accuracy;Electricity;Simulation;Data models;Smart grids;Convolutional neural networks;Security;Context modeling;Resilience;Testing;Smart Grid;Adversarial Examples;Evasion Attack;Energy Theft Detection System},
  doi={10.1109/ICISAA62385.2024.10829076}}
  @article{https://doi.org/10.1155/2024/1124598,
author = {Lee, Jun and Kim, Taewan and Bang, Seungho and Oh, Sehong and Kwon, Hyun},
title = {Evasion Attacks on Deep Learning-Based Helicopter Recognition Systems},
journal = {Journal of Sensors},
volume = {2024},
number = {1},
pages = {1124598},
doi = {https://doi.org/10.1155/2024/1124598},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2024/1124598},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2024/1124598},
abstract = {Identifying objects in surveillance and reconnaissance systems with the human eye can be challenging, underscoring the growing importance of employing deep learning models for the recognition of enemy weapon systems. These systems, leveraging deep neural networks known for their strong performance in image recognition and classification, are currently under extensive research. However, it is crucial to acknowledge that surveillance and reconnaissance systems utilizing deep neural networks are susceptible to vulnerabilities posed by adversarial examples. While prior adversarial example research has mainly utilized publicly available internet data, there has been a significant absence of studies concerning adversarial attacks on data and models specific to real military scenarios. In this paper, we introduce an adversarial example designed for a binary classifier tasked with recognizing helicopters. Our approach generates an adversarial example that is misclassified by the model, despite appearing unproblematic to the human eye. To conduct our experiments, we gathered real attack and transport helicopters and employed TensorFlow as the machine learning library of choice. Our experimental findings demonstrate that the average attack success rate of the proposed method is 81.9\%. Additionally, when epsilon is 0.4, the attack success rate is 90.1\%. Before epsilon reaches 0.4, the attack success rate increases rapidly, and then we can see that epsilon increases little by little thereafter.},
year = {2024}
}
@article{MARULLI20213570,
title = {Exploring Data and Model Poisoning Attacks to Deep Learning-Based NLP Systems},
journal = {Procedia Computer Science},
volume = {192},
pages = {3570-3579},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.130},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101869X},
author = {Fiammetta Marulli and Laura Verde and Lelio Campanile},
keywords = {Natural Language Processing, Deep Learning Vulnerabilities, Data Poisoning Attacks, Poisoned Word Embeddings, Reliable Machine Learning},
abstract = {Natural Language Processing (NLP) is being recently explored also to its application in supporting malicious activities and objects detection. Furthermore, NLP and Deep Learning have become targets of malicious attacks too. Very recent researches evidenced that adversarial attacks are able to affect also NLP tasks, in addition to the more popular adversarial attacks on deep learning systems for image processing tasks. More precisely, while small perturbations applied to the data set adopted for training typical NLP tasks (e.g., Part-of-Speech Tagging, Named Entity Recognition, etc..) could be easily recognized, models poisoning, performed by the means of altered data models, typically provided in the transfer learning phase to a deep neural networks (e.g., poisoning attacks by word embeddings), are harder to be detected. In this work, we preliminary explore the effectiveness of a poisoned word embeddings attack aimed at a deep neural network trained to accomplish a Named Entity Recognition (NER) task. By adopting the NER case study, we aimed to analyze the severity of such a kind of attack to accuracy in recognizing the right classes for the given entities. Finally, this study represents a preliminary step to assess the impact and the vulnerabilities of some NLP systems we adopt in our research activities, and further investigating some potential mitigation strategies, in order to make these systems more resilient to data and models poisoning attacks.}
}
@misc{wu2020evaluationinferenceattackmodels,
      title={Evaluation of Inference Attack Models for Deep Learning on Medical Data}, 
      author={Maoqiang Wu and Xinyue Zhang and Jiahao Ding and Hien Nguyen and Rong Yu and Miao Pan and Stephen T. Wong},
      year={2020},
      eprint={2011.00177},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.00177}, 
}
@InProceedings{pmlr-v139-choquette-choo21a,
  title = 	 {Label-Only Membership Inference Attacks},
  author =       {Choquette-Choo, Christopher A. and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1964--1974},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/choquette-choo21a/choquette-choo21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/choquette-choo21a.html},
  abstract = 	 {Membership inference is one of the simplest privacy threats faced by machine learning models that are trained on private sensitive data. In this attack, an adversary infers whether a particular point was used to train the model, or not, by observing the model’s predictions. Whereas current attack methods all require access to the model’s predicted confidence score, we introduce a label-only attack that instead evaluates the robustness of the model’s predicted (hard) labels under perturbations of the input, to infer membership. Our label-only attack is not only as-effective as attacks requiring access to confidence scores, it also demonstrates that a class of defenses against membership inference, which we call “confidence masking” because they obfuscate the confidence scores to thwart attacks, are insufficient to prevent the leakage of private information. Our experiments show that training with differential privacy or strong L2 regularization are the only current defenses that meaningfully decrease leakage of private information, even for points that are outliers of the training distribution.}
}

@inproceedings{Hu_2020, series={ASPLOS ’20},
   title={DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints},
   url={http://dx.doi.org/10.1145/3373376.3378460},
   DOI={10.1145/3373376.3378460},
   booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
   publisher={ACM},
   author={Hu, Xing and Liang, Ling and Li, Shuangchen and Deng, Lei and Zuo, Pengfei and Ji, Yu and Xie, Xinfeng and Ding, Yufei and Liu, Chang and Sherwood, Timothy and Xie, Yuan},
   year={2020},
   month=mar, pages={385–399},
   collection={ASPLOS ’20} }
